{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5d5730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "\n",
    "%reload_ext tensorboard\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pickle\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bab71375",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MicroclimateData = pd.read_csv('./Data/all_buildings_limited.csv')\n",
    "WeatherStationData = pd.read_csv('./Data/all_buildings_big.csv')\n",
    "def undummify(df, prefix_sep=\"_\"):\n",
    "    cols2collapse = {\n",
    "        item.split(prefix_sep)[0]: (prefix_sep in item) for item in df.columns\n",
    "    }\n",
    "    series_list = []\n",
    "    for col, needs_to_collapse in cols2collapse.items():\n",
    "        if needs_to_collapse:\n",
    "            undummified = (\n",
    "                df.filter(like=col)\n",
    "                .idxmax(axis=1)\n",
    "                .apply(lambda x: x.split(prefix_sep, maxsplit=1)[1])\n",
    "                .rename(col)\n",
    "            )\n",
    "            series_list.append(undummified)\n",
    "        else:\n",
    "            series_list.append(df[col])\n",
    "    undummified_df = pd.concat(series_list, axis=1)\n",
    "    return undummified_df\n",
    "def plot_predictions(MicroclimateModel,text, mod_type):\n",
    "    date_time = MicroclimateData[['Date_Time',\n",
    "                        'Month',\n",
    "                        'Day',\n",
    "                        'Hour',\n",
    "                        'Minute']]\n",
    "\n",
    "    x1,x2,y1,y2 = MicroclimateModel.prepare_data(MicroclimateData,'train')\n",
    "    # Microclimate Model(NN)\n",
    "    comb = [x1,x2]\n",
    "    x1 = pd.concat(comb)\n",
    "    Y_preds_original = MicroclimateModel.model.predict(x1)\n",
    "    results_NN = undummify(x1).join(date_time)\n",
    "    col = results_NN.pop('bldgname')\n",
    "    results_NN.insert(0, 'bldgname', col)\n",
    "    y1= np.concatenate((y1,y2))\n",
    "    results_NN['Actual'] = MicroclimateModel.scaler.inverse_transform(y1)\n",
    "    results_NN['MicroModel Baseline'] = MicroclimateModel.scaler.inverse_transform(Y_preds_original)     \n",
    "    Buildings = results_NN['bldgname'].unique()    \n",
    "    for bldg in Buildings:\n",
    "        if mod_type == '0':\n",
    "            string = './Data/Plots/sensitivity analysis/WithoutAnyMod/'+ send_text + bldg  \n",
    "            x3,x4,y3,y4 = MicroclimateModel.modify_data(MicroclimateData, mod_type)\n",
    "            comb1 = [x3,x4]\n",
    "            x3 = pd.concat(comb1)\n",
    "            Y_preds_new = MicroclimateModel.model.predict(x3)\n",
    "            y3= np.concatenate((y3,y4))       \n",
    "            results_NN['Time'] = results_NN['Date_Time'].str[11:16]\n",
    "            results_NN['Predicted'] = MicroclimateModel.scaler.inverse_transform(Y_preds_new)        \n",
    "            month_NN = results_NN.loc[results_NN['Month'] == 8]\n",
    "            day_NN = month_NN.loc[month_NN['Day'] == 1]\n",
    "            bldg_NN = day_NN.loc[results_NN['bldgname']==bldg]\n",
    "            values_NN = bldg_NN.sort_values(by=['Time'])\n",
    "            values_NN['MicroModel Trained without'+ ' any modifications'] = values_NN['Predicted']\n",
    "            values_NN['MicroModel Baseline'] = values_NN['Microclimate_baseline']            \n",
    "            ax = values_NN.plot(x='Time',y='Actual', figsize = (8,8))\n",
    "            values_NN.plot(x='Time',y='MicroModel Baseline',figsize = (8,8),ax=ax)\n",
    "            plt.savefig(string, bbox_inches='tight')\n",
    "        if mod_type == '1':\n",
    "            string = './Data/Plots/sensitivity analysis/WithTemperatureMod/'+ send_text + bldg\n",
    "            x3,x4,y3,y4 = MicroclimateModel.modify_data(MicroclimateData, mod_type)\n",
    "            comb1 = [x3,x4]\n",
    "            x3 = pd.concat(comb1)\n",
    "            Y_preds_new = MicroclimateModel.model.predict(x3)\n",
    "            y3= np.concatenate((y3,y4))       \n",
    "            results_NN['Time'] = results_NN['Date_Time'].str[11:16]\n",
    "            results_NN['Predicted'] = MicroclimateModel.scaler.inverse_transform(Y_preds_new)        \n",
    "            month_NN = results_NN.loc[results_NN['Month'] == 8]\n",
    "            day_NN = month_NN.loc[month_NN['Day'] == 1]\n",
    "            bldg_NN = day_NN.loc[results_NN['bldgname']==bldg]\n",
    "            values_NN = bldg_NN.sort_values(by=['Time'])\n",
    "            values_NN['MicroModel Predictions after reducing temperature by 1 degree'] = values_NN['Predicted']\n",
    "            ax = values_NN.plot(x='Time',y='Actual',grid = False, figsize = (8,8))\n",
    "            values_NN.plot(x='Time',y='MicroModel Baseline',grid = False, figsize = (8,8),ax=ax)\n",
    "            values_NN.plot(x='Time',y='MicroModel Predictions after reducing temperature by 1 degree',grid = False, figsize = (8,8),ax=ax)\n",
    "            plt.savefig(string, bbox_inches='tight')\n",
    "        if mod_type == '2':\n",
    "            string = './Data/Plots/sensitivity analysis/WithShadeMod/'+ send_text + bldg\n",
    "            x3,x4,y3,y4 = MicroclimateModel.modify_data(MicroclimateData, mod_type)\n",
    "            comb1 = [x3,x4]\n",
    "            x3 = pd.concat(comb1)\n",
    "            Y_preds_new = MicroclimateModel.model.predict(x3)\n",
    "            y3= np.concatenate((y3,y4))       \n",
    "            results_NN['Time'] = results_NN['Date_Time'].str[11:16]\n",
    "            results_NN['Predicted'] = MicroclimateModel.scaler.inverse_transform(Y_preds_new)        \n",
    "            month_NN = results_NN.loc[results_NN['Month'] == 8]\n",
    "            day_NN = month_NN.loc[month_NN['Day'] == 1]\n",
    "            bldg_NN = day_NN.loc[results_NN['bldgname']==bldg]\n",
    "            values_NN = bldg_NN.sort_values(by=['Time'])\n",
    "            values_NN['MicroModel Predictions after increasing Shade by 25%'] = values_NN['Predicted']\n",
    "            ax = values_NN.plot(x='Time',y='Actual',grid = False, figsize = (8,8))\n",
    "            values_NN.plot(x='Time',y='MicroModel Baseline',grid = False, figsize = (8,8),ax=ax)\n",
    "            values_NN.plot(x='Time',y='MicroModel Predictions after increasing Shade by 25%',grid = False, figsize = (8,8),ax=ax)\n",
    "            plt.savefig(string, bbox_inches='tight')\n",
    "        if mod_type == '3':\n",
    "            string = './Data/Plots/sensitivity analysis/WithBothMod/'+ send_text + bldg\n",
    "            x3,x4,y3,y4 = MicroclimateModel.modify_data(MicroclimateData, '1') #with temperature decrease\n",
    "            comb1 = [x3,x4]\n",
    "            x3 = pd.concat(comb1)\n",
    "            Y_preds_new = MicroclimateModel.model.predict(x3)\n",
    "            results_NN['Predicted_1'] = MicroclimateModel.scaler.inverse_transform(Y_preds_new)  \n",
    "            x3,x4,y3,y4 = MicroclimateModel.modify_data(MicroclimateData, '2') #with shade increase\n",
    "            comb1 = [x3,x4]\n",
    "            x3 = pd.concat(comb1)\n",
    "            Y_preds_new = MicroclimateModel.model.predict(x3)\n",
    "            results_NN['Predicted_2'] = MicroclimateModel.scaler.inverse_transform(Y_preds_new)    \n",
    "            results_NN['Time'] = results_NN['Date_Time'].str[11:16]            \n",
    "            month_NN = results_NN.loc[results_NN['Month'] == 8]\n",
    "            day_NN = month_NN.loc[month_NN['Day'] == 1]\n",
    "            bldg_NN = day_NN.loc[results_NN['bldgname']==bldg]\n",
    "            values_NN = bldg_NN.sort_values(by=['Time'])            \n",
    "            values_NN['MicroModel Predictions after reducing temperature by 1 degree'] = values_NN['Predicted_1'] \n",
    "            values_NN['MicroModel Predictions after increasing shade'] = values_NN['Predicted_2'] \n",
    "#             print(values_NN['Predicted_1'])\n",
    "#             print(values_NN['Predicted_2'])\n",
    "            ax = values_NN.plot(x='Time',y='Actual',grid = False, figsize = (8,8))\n",
    "            values_NN.plot(x='Time',y='MicroModel Baseline',grid = False, figsize = (8,8),ax=ax)          \n",
    "            values_NN.plot(x='Time',y='MicroModel Predictions after reducing temperature by 1 degree',grid = False, figsize = (8,8),ax=ax)\n",
    "            values_NN.plot(x='Time',y='MicroModel Predictions after increasing shade',grid = False, figsize = (8,8),ax=ax)\n",
    "            plt.savefig(string, bbox_inches='tight')    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69d2306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self,activation_type, hidden_layers, loss, kernel_initializer, bias_initializer, data_type,epochs, modify_type):\n",
    "        self.activation_type = activation_type\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.loss = loss\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer \n",
    "        self.data_type = data_type\n",
    "        self.epochs = epochs\n",
    "        self.modify_type = modify_type\n",
    "    def huber_loss(y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "        squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "        linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "        return tf.where(cond, squared_loss, linear_loss)\n",
    "    def huber_loss_mean(y_true, y_pred, clip_delta=1.0):\n",
    "        return tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_delta))\n",
    "    def prepare_data(self, dataset, func):        \n",
    "        #add artificial features\n",
    "        dataset['Air Temp Squared'] =  dataset['Air Temp']**2\n",
    "        dataset['Abs Hum Squared'] =  dataset['Abs Hum']**2\n",
    "        dataset['Air Temp*Abs Hum'] =  dataset['Air Temp']* dataset['Abs Hum']\n",
    "        MicroclimateData['Air Temp Squared'] =  MicroclimateData['Air Temp']**2\n",
    "        MicroclimateData['Abs Hum Squared'] =  MicroclimateData['Abs Hum']**2\n",
    "        MicroclimateData['Air Temp*Abs Hum'] =  MicroclimateData['Air Temp']* MicroclimateData['Abs Hum']\n",
    "        if self.data_type == 'WeatherStation'  and (func == 'test' or func == 'train'):\n",
    "            X = dataset[['bldgname',\n",
    "                        'Air Temp', \n",
    "                        'Abs Hum',\n",
    "                        'Air Temp Squared',\n",
    "                        'Abs Hum Squared',\n",
    "                        'Air Temp*Abs Hum']]\n",
    "            columns = ['Air Temp', 'Abs Hum',\n",
    "                       'Air Temp Squared',\n",
    "                        'Abs Hum Squared',\n",
    "                        'Air Temp*Abs Hum']\n",
    "            Y = dataset['CHWTON/SQFT']\n",
    "        if self.data_type == 'Microclimate' and func == 'train':\n",
    "            X = dataset[['bldgname',\n",
    "                                  'Air Temp', \n",
    "                                  'Abs Hum',\n",
    "                                  'Air Temp Squared',\n",
    "                                  'Abs Hum Squared',\n",
    "                                  'Air Temp*Abs Hum',\n",
    "                                  'DSW Top', \n",
    "                                  'DSW North', \n",
    "                                  'DSW South', \n",
    "                                  'DSW East', \n",
    "                                  'DSW West', \n",
    "                                  'Shade North', \n",
    "                                  'Shade East', \n",
    "                                  'Shade West',\n",
    "                                  'Shade South']]\n",
    "            columns = ['Air Temp', 'Abs Hum', 'Air Temp Squared',\n",
    "                        'Abs Hum Squared', 'Air Temp*Abs Hum', 'DSW Top',\n",
    "                       'DSW North', 'DSW South', 'DSW East','DSW West',\n",
    "                       'Shade North', 'Shade East', 'Shade West', 'Shade South',]\n",
    "            Y = dataset['CHWTON/SQFT']\n",
    "        if self.data_type == 'Microclimate' and func == 'test':\n",
    "            X = dataset[['bldgname',\n",
    "                        'Air Temp', \n",
    "                        'Abs Hum',\n",
    "                        'Air Temp Squared',\n",
    "                        'Abs Hum Squared',\n",
    "                        'Air Temp*Abs Hum']]\n",
    "            columns = ['Air Temp', 'Abs Hum',\n",
    "                       'Air Temp Squared',\n",
    "                        'Abs Hum Squared',\n",
    "                        'Air Temp*Abs Hum']\n",
    "            Y = dataset['CHWTON/SQFT']\n",
    "        Y = Y.values.reshape(-1, 1)\n",
    "        self.scaler = StandardScaler()        \n",
    "        Y = self.scaler.fit_transform(Y)\n",
    "        X = pd.get_dummies(X)\n",
    "        StdSc2 = StandardScaler()\n",
    "        X[columns] = StdSc2.fit_transform(X[columns])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=20)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    def modify_data(self,dataset, mod_type):\n",
    "        if mod_type == '1':\n",
    "            dataset['Air Temp'] = dataset['Air Temp'] - 1\n",
    "            print(\"modify\")\n",
    "        elif mod_type == '0':\n",
    "            dataset = dataset\n",
    "        elif mod_type == '2':\n",
    "            print(\"modify\")\n",
    "            shade_list_north = dataset['Shade North'].tolist()\n",
    "            shade_list_north = [x + x*0.25 if int(x)!=1.0 else x for x in shade_list_north] \n",
    "            dataset.drop('Shade North', axis = 1, inplace = True)\n",
    "            dataset['Shade North'] = shade_list_north  \n",
    "            shade_list_south = dataset['Shade South'].tolist()\n",
    "            shade_list_south = [x + x*0.25 if int(x)!=1.0 else x  for x in shade_list_south]\n",
    "            dataset.drop('Shade South', axis = 1, inplace = True)\n",
    "            dataset['Shade South'] = shade_list_south\n",
    "            shade_list_east = dataset['Shade East'].tolist()\n",
    "            shade_list_east = [x + x*0.25 if int(x)!=1.0 else x  for x in shade_list_east]\n",
    "            dataset.drop('Shade East', axis = 1, inplace = True)\n",
    "            dataset['Shade East'] = shade_list_east   \n",
    "            shade_list_west = dataset['Shade West'].tolist()\n",
    "            shade_list_west = [x + x*0.25 if int(x)!=1.0 else x  for x in shade_list_west]\n",
    "            dataset.drop('Shade West', axis = 1, inplace = True)\n",
    "            dataset['Shade West'] = shade_list_west\n",
    "        X_train,X_test,y_train, y_test  = self.prepare_data(dataset,'train')\n",
    "        return X_train, X_test, y_train, y_test \n",
    "    def define_model(self):\n",
    "        if self.data_type == 'WeatherStation' :\n",
    "            model = Sequential()\n",
    "            if self.activation_type == 'leaky_relu':\n",
    "                model.add(Dense(20, \n",
    "                                input_dim=16, \n",
    "                                kernel_initializer=self.kernel_initializer, \n",
    "                                bias_initializer=self.bias_initializer, \n",
    "                                activation= self.activation_type))\n",
    "                for i in range(0,(self.hidden_layers-2)):\n",
    "                    model.add(Dense(30, activation = self.activation_type))\n",
    "                model.add(Dense(50, activation = self.activation_type))\n",
    "                model.add(Dropout(0.05))\n",
    "                model.add(Dense(1, activation = 'linear'))\n",
    "            else:\n",
    "                model.add(Dense(30, \n",
    "                                input_dim=16, \n",
    "                                kernel_initializer=self.kernel_initializer, \n",
    "                                bias_initializer=self.bias_initializer, \n",
    "                                activation= self.activation_type))\n",
    "                for i in range(0,(self.hidden_layers-2)):\n",
    "                    model.add(Dense(40, activation = self.activation_type))\n",
    "                model.add(Dense(50, activation = self.activation_type))\n",
    "                model.add(Dropout(0.05))\n",
    "                model.add(Dense(1, activation = 'linear'))           \n",
    "        if self.data_type == 'Microclimate':\n",
    "            model = Sequential()\n",
    "            if self.activation_type == 'leaky_relu':\n",
    "                model.add(Dense(30, \n",
    "                                input_dim=25, \n",
    "                                kernel_initializer=self.kernel_initializer, \n",
    "                                bias_initializer=self.bias_initializer, \n",
    "                                activation= self.activation_type))\n",
    "                for i in range(0,(self.hidden_layers-2)):\n",
    "                    model.add(Dense(40, activation = self.activation_type))\n",
    "                model.add(Dense(50, activation = self.activation_type))\n",
    "                model.add(Dropout(0.05))\n",
    "                model.add(Dense(1, activation = 'linear'))\n",
    "            else:\n",
    "                model.add(Dense(30, \n",
    "                                input_dim=25, \n",
    "                                kernel_initializer=self.kernel_initializer, \n",
    "                                bias_initializer=self.bias_initializer, \n",
    "                                activation= self.activation_type))\n",
    "                for i in range(0,(self.hidden_layers-2)):\n",
    "                    model.add(Dense(40, activation = self.activation_type))\n",
    "                model.add(Dense(50, activation = self.activation_type))\n",
    "                model.add(Dropout(0.05))\n",
    "                model.add(Dense(1, activation = 'linear'))\n",
    "        model.compile( loss = self.loss, \n",
    "                      optimizer = 'adam')\n",
    "        model.summary()\n",
    "        return model\n",
    "    def train(self, dataset):\n",
    "#         dataset_mod = self.modify_dataset(dataset,self.modify_type)\n",
    "        X_train, X_test, y_train, y_test = self.prepare_data(dataset,'train')        \n",
    "        self.model = self.define_model()\n",
    "        # Define the Keras TensorBoard callback.\n",
    "        logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "        self.history = self.model.fit(X_train,y_train, \n",
    "                            batch_size=128, \n",
    "                            epochs=self.epochs, \n",
    "                            verbose=0, \n",
    "                            validation_split=0.2, \n",
    "                            callbacks=[tensorboard_callback])\n",
    "    def fine_tune(self, dataset,alpha=0.001):\n",
    "        self.model.trainable = True\n",
    "        fine_tune_at = 2\n",
    "        for layer in self.model.layers[:fine_tune_at]:\n",
    "            layer.trainable =  False\n",
    "        opt= Adam(learning_rate=alpha)\n",
    "        self.model.compile(loss=self.loss, \n",
    "                           optimizer=opt)\n",
    "        X_train, X_test, y_train, y_test = self.prepare_data(dataset,'test')\n",
    "        # Define the Keras TensorBoard callback.\n",
    "        logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "        self.history = self.model.fit(X_train,y_train, \n",
    "                            batch_size=128, \n",
    "                            epochs=self.epochs, \n",
    "                            verbose=0, \n",
    "                            validation_split=0.2, \n",
    "                            callbacks=[tensorboard_callback])\n",
    "    def plot_curve_org(self,text):\n",
    "        plt.plot(self.history.history['loss'])\n",
    "        plt.plot(self.history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        save_name = \"./Data/plots-o/\" + text + \".png\"\n",
    "        plt.savefig(save_name, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    def plot_curve(self,text):\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        ax.plot(self.history.history[\"loss\"],'r-x', label=\"Train Loss\")\n",
    "        ax.plot(self.history.history[\"val_loss\"],'b-x', label=\"Validation Loss\")\n",
    "        ax.legend()\n",
    "        ax.set_title(text)\n",
    "        ax.grid(True)    \n",
    "        save_name = \"./Data/plots/\" + text + \".png\"\n",
    "        plt.savefig(save_name, bbox_inches='tight')\n",
    "  \n",
    "    def evaluate(self, dataset, data_type ):\n",
    "        call_type = 'test'\n",
    "        if data_type == self.data_type:\n",
    "            call_type = 'train'\n",
    "        _, X_test, _, y_test = self.prepare_data(dataset,call_type)\n",
    "        Y_preds = self.model.predict(X_test)\n",
    "        RMSE = np.sqrt(metrics.mean_squared_error(y_test, Y_preds))\n",
    "        R2_score = metrics.r2_score(y_test, Y_preds)\n",
    "        print(\"The RMSE score for test dataset of \"+ data_type +\" is:\", RMSE)\n",
    "        print(\"The R2 score for test dataset of \"+ data_type +\" is:\", R2_score)\n",
    "        return RMSE,R2_score\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4df4f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 30)                780       \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 40)                1240      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 50)                2050      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 5,761\n",
      "Trainable params: 5,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 23:07:05.170996: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2021-11-15 23:07:05.171051: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0011s vs `on_train_batch_end` time: 0.0220s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 23:07:05.527910: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.\n",
      "2021-11-15 23:07:05.527964: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2021-11-15 23:07:05.548105: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2021-11-15 23:07:05.549294: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05\n",
      "2021-11-15 23:07:05.549901: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.trace.json.gz\n",
      "2021-11-15 23:07:05.550679: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05\n",
      "2021-11-15 23:07:05.550723: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.memory_profile.json.gz\n",
      "2021-11-15 23:07:05.550865: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05Dumped tool data for xplane.pb to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/fit/20211115-230705/train/plugins/profile/2021_11_15_23_07_05/biswas.kernel_stats.pb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Train microclimate Model and save results for each parameter modification\n",
    "    MicroclimateModel = NeuralNetwork('relu',4, 'huber_loss', 'he_normal','he_normal', 'Microclimate', 200, 0)\n",
    "    MicroclimateModel.train(MicroclimateData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4309a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modify\n",
      "modify\n"
     ]
    }
   ],
   "source": [
    "# send_text = 'Microclimate Model with reducing temperature by 1 degree' #modify as required\n",
    "send_text = \"Microclimate Predicted with  new data\"\n",
    "#             MicroclimateModel.plot_curve(send_text)\n",
    "plot_predictions(MicroclimateModel,send_text,'3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf6e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
